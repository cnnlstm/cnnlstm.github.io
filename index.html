<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />

<meta name="keywords" content="Yangyang, Yangyang Xu, CS, HKU, The University of Hong Kong">
<meta name="description" content="Yuanfeng Ji&#39;s home page">
<meta name="google-site-verification" content="X2QFrl-bPeg9AdlMt4VKT9v6MJUSTCf-SrY3CvKt4Zs" />
<link rel="stylesheet" href="jemdoc.css" type="text/css">
<link rel="shortcut icon" type="image/png" href="images/tab.ico" />

<title>Yangyang Xu&#39;s Homepage</title>
<!-- Google Analytics -->
<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-159069803-1', 'auto');
ga('send', 'pageview');
</script>
<!-- End Google Analytics -->
<!--
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-87320911-1', 'auto');
  ga('send', 'pageview');

</script>
-->
</head>
<body>
<div id="layout-content" style="margin-top:25px">
<table>
	<tbody>
		<tr>
			<td width="650">
				<div id="toptitle">
					<h1>Yangyang Xu</h1><h1>
					<h1><font face="Arial"> 徐洋洋 </font></h1>
				</h1></div>

				<h3>PostDoc.</h3>
				<p>
					Dept. of Computer Science <br>
					The University of Hong Kong <br>
					Pokfulam, Hong Kong<br>
<!-- 					<br>
					Email: cnnlstm[at]gmail.com -->

		<p style="text-align:left">
    <a href="mailto:cnnlstm@gmail.com"> Email</a> &nbsp/&nbsp
    <a href="https://scholar.google.com/citations?user=SmlxBFAAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
    <a href="https://cnnlstm.github.io/YangyangXu_cv_open.pdf">CV</a> &nbsp/&nbsp
    <a href="https://cnnlstm.github.io/images/wechat.jpg">WeChat</a>

  	</p>
				</p>
				</p>
			</td>
			<td>
				<img src="./images/635.jpg" border="0" width="150"><br>
			</td>
		</tr><tr>
	</tr></tbody>
</table>

<!--<h2>Biography [<a href="./CV-JinYueming.pdf">CV</a>]</h2>-->
<h2>Biography </h2>
<p>
	I am currently working as a postdoctoral with Prof. <a href="http://luoping.me/">Ping Luo</a> at <a href="https://www.hku.hk/">The University of Hong Kong</a>. I am also an adjunct researcher at <a href="https://www.shlab.org.cn/">Shanghai AI Laboratory</a>, working with Prof. <a href="http://mmlab.siat.ac.cn/yuqiao/">Yu Qiao</a> and Prof. <a href="https://daibo.info/">Bo Dai</a>. Before that, I receive my doctorate degree under the supervision of Prof. <a href="https://shengfenghe.com/">Shengfeng He</a> and Prof. <a href="https://scut-mm.github.io/people.html/">Xuemiao Xu</a> at the South China University of Technology(<a href="https://www.scut.edu.cn/en/">SCUT</a>).
</p>
<p>
	My research mainly focuses on <b>generative models and their applications</b>, including image/video generation, editing, translation, and super-resolution. I am also interested in image/video matting, zero/few-shot learning, and 2D/3D action recognition.
</p>
	<h2>News</h2>
<ul>
		<li>07/2023: One Paper is accepted by ICCV. </li>
		<li>02/2023: One Paper is accepted by TOG. </li>
		<li>07/2022: One Paper is accepted by TIP. </li>
    <li>03/2022: One Paper is accepted by CVPR. </li>
    <li>12/2021: One Paper is accepted by TIP. </li>
    <li>12/2021: Join the department of computer science, HKU, and start the postdoctoral phase.</li>
    <li>08/2021: Passed the oral defense and became a <span style="color:rgb(224, 145, 92)"><b>Dr</b></span>.</li>
    <li>07/2021: One Paper is accepted by ICCV.</li>
    <li>06/2021: One Paper is accepted by TIP.</li>

</ul>


<h2><font> Publications </font> </h2>
<table id="tbPublications" width="100%">
	<tbody>

		<p>Summary: CVPR & ICCV (3),  <i>IEEE</i> & <i>ACM</i> Transactions (10). </font> </p>

    <p> * Corresponding author; + Equal Contribution </p>

    <tr>
		<td width="206">
		<img src="images/iccv2.png" width="200px" height = "75	" style="box-shadow: 4px 4px 8px #888">
		</td>
		<td><b>RIGID: Recurrent GAN Inversion and Editing of Real Face Videos</b> <br>
		<b>Yangyang Xu</b>,  Shengfeng He, Kwan-Yee K. Wong, and Ping Luo<br>
		<i><b>ICCV 2023</b></i>  
		<br>
		<a href="https://arxiv.org/pdf/2308.06097.pdf">PDF</a> / 
		Supp / 
		<a href="https://cnnlstm.github.io/RIGID/">Project</a>  
		</td>

    <tr>
		<td width="206">
		<img src="images/tog1.png" width="200px" height = "75	" style="box-shadow: 4px 4px 8px #888">
		</td>
		<td><b>Parsing-Conditioned Anime Translation: A New Dataset and Method</b> <br>
		Zhansheng Li+, <b>Yangyang Xu</b>+,  Nanxuan Zhao, Yang Zhou, Yongtuo Liu, Dahua Lin, and Shengfeng He<br>
		<i><b>ACM TOG 2023</b></i>  
		<br>
		<a href="https://dl.acm.org/doi/10.1145/3585002">PDF</a> / 
		Supp / 
		<a href="https://github.com/zsl2018/StyleAnime/">Code</a>  
		</td>


    <tr>
		<td width="206">
		<img src="images/mat.png" width="200px" height = "75	" style="box-shadow: 4px 4px 8px #888">
		</td>
		<td><b>Self-supervised Matting-specific Portrait Enhancement and Generation</b> <br>
		<b>Yangyang Xu</b>,  Zeyang Zhou and Shengfeng He<br>
		<i><b>IEEE TIP 2022</b></i>  
		<br>
		<a href="https://ieeexplore.ieee.org/document/9849440">PDF</a> / 
		Supp / 
		<a href="https://github.com/cnnlstm/StyleGAN_Matting">Code</a>  
		</td>


    <tr>
		<td width="206">
		<img src="images/faceswap.png" width="200px" height = "75	" style="box-shadow: 4px 4px 8px #888">
		</td>
		<td><b>High-resolution Face Swapping via Latent Semantics Disentanglement</b> <br>
		<b>Yangyang Xu</b>,  Bailin Deng, Junle Wang, Yanqing Jing, Jia Pan and Shengfeng He<br>
		<i><b>CVPR 2022</b></i>  
		<br>
		<a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Xu_High-Resolution_Face_Swapping_via_Latent_Semantics_Disentanglement_CVPR_2022_paper.pdf">PDF</a> / 
		<a href="https://openaccess.thecvf.com/content/CVPR2022/supplemental/Xu_High-Resolution_Face_Swapping_CVPR_2022_supplemental.pdf">Supp</a> / 
		<a href="https://github.com/cnnlstm/FSLSD_HiRes">Code</a>  
		</td>


		<tr>
		<td width="206">
		<img src="images/propulse.png" width="200px" height = "75	" style="box-shadow: 4px 4px 8px #888">
		</td>
		<td><b>Pro-PULSE: Learning Progressive Encoders of Latent Semantics in GANs for Photo Upsampling</b> <br>
		Yang Zhou+, <b>Yangyang Xu</b>+, Yong Du, Qiang Wen and Shengfeng He<br>
		<i><b>IEEE TIP 2022</b></i>  
		<br>
    <a href="https://ieeexplore.ieee.org/document/9678071">PDF</a>  /
    Supp / 
    Code 
		</td>


		<tr>
		<td width="206">
		<img src="images/icme.png" width="200px" height = "75	" style="box-shadow: 4px 4px 8px #888">
		</td>
		<td><b>Background Matting via Recursive Excitation</b> <br>
		Junjie Deng+, <b>Yangyang Xu</b>+, Zeyang Zhou and Shengfeng He<br>
		<i><b>ICME 2022</b></i>  
		<br>
    <a href="https://ieeexplore.ieee.org/document/9859876/">PDF</a>  / 
    Supp / 
    <a href="https://github.com/csdjj/EnhancedBGM/">Code</a> 
		</td>

		<tr>
		<td width="206">
		<img src="images/aod.png" width="200px" height = "75	" style="box-shadow: 4px 4px 8px #888">
		</td>
		<td><b>Representative Feature Alignment for Adaptive Object Detection</b> <br>
		Shan Xu, Huaidong Zhang, Xuemiao Xu, Xiaowei Hu, <b>Yangyang Xu</b>, Liangui Dai, Kup-Sze Choi, and Pheng-Ann Heng<br>
		<i><b>IEEE TCSVT 2022</b></i>  
		<br>
    <a href="https://ieeexplore.ieee.org/abstract/document/9868052/">PDF</a>  / 
    Supp / 
    Code
		</td>


		<tr>
		<td width="206">
		<img src="images/inversion.png" width="200px" height = "75	" style="box-shadow: 4px 4px 8px #888">
		</td>
		<td><b>From Continuity to Editability: Inverting GANs with Consecutive Images</b> <br>
		<b>Yangyang Xu</b>, Yong Du, Wenpeng Xiao, Xuemiao Xu, and Shengfeng He<br>
		<i><b>ICCV 2021</b></i>  
		<br>
    <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Xu_From_Continuity_to_Editability_Inverting_GANs_With_Consecutive_Images_ICCV_2021_paper.pdf">PDF</a> / 
    <a href="https://openaccess.thecvf.com/content/ICCV2021/supplemental/Xu_From_Continuity_to_ICCV_2021_supplemental.pdf">Supp</a> / 
    <a href="https://github.com/cnnlstm/InvertingGANs_with_ConsecutiveImgs">Code</a>  
		</td>


		<tr>
		<td width="206">
		<img src="images/faceflow.png" width="200px" height = "75	" style="box-shadow: 4px 4px 8px #888">
		</td>
		<td><b>Multi-view Face Synthesis via Progressive Face Flow</b> <br>
		<b>Yangyang Xu</b>, Xuemiao Xu, Jianbo Jiao, Keke Li, Cheng Xu and Shengfeng He<br>
		<i><b>IEEE TIP 2021</b></i>  
		<br>
    <a href="https://ieeexplore.ieee.org/document/9466401">PDF</a> / 
    <a href="https://ieeexplore.ieee.org/document/9495995">Erratum</a> / 
    Code      
		</td>


		<tr>
		<td width="206">
		<img src="images/ham.png" width="200px" height = "75	" style="box-shadow: 4px 4px 8px #888">
		</td>
		<td><b>Holistically-Associated Transductive Zero-Shot Learning</b> <br>
		<b>Yangyang Xu</b>, Xuemiao Xu, Guoqiang Han, and Shengfeng He<br>
		<i><b>IEEE TCDS 2021</b></i>  
		<br>
    <a href="https://ieeexplore.ieee.org/document/9314882">PDF</a> / 
    Supp / 
    Code 
		</td>


		<tr>
		<td width="206">
		<img src="images/tomm.png" width="200px" height = "75	" style="box-shadow: 4px 4px 8px #888">
		</td>
		<td><b>Invertible Grayscale with Sparsity Enforcing Priors</b> <br>
		Yong Du, <b>Yangyang Xu</b>, Taizhong Ye, Qiang Wen, Chufeng Xiao, Junyu Dong, Guoqiang Han, Shengfeng He<br>
		<i><b>ACM TOMM 2021</b></i>  
		<br>
    <a href="https://dl.acm.org/doi/10.1145/3451993">PDF</a> / 
    Supp / 
    Code 
		</td>

		<tr>
		<td width="206">
		<img src="images/cod.png" width="200px" height = "75	" style="box-shadow: 4px 4px 8px #888">
		</td>
		<td><b>Deep Texture-Aware Features for Camouflaged Object Detection</b> <br>
		Jingjing Ren, Xiaowei Hu, Lei Zhu, Xuemiao Xu, <b>Yangyang Xu</b>, Weiming Wang, Zijun Deng and Pheng-Ann Heng<br>
		<i><b>IEEE TCSVT 2021</b></i>  
		<br>
    <a href="https://ieeexplore.ieee.org/document/9606888">PDF</a> / 
    Supp / 
    Code 
		</td>

		<tr>
		<td width="206">
		<img src="images/gagcn.png" width="200px" height = "75	" style="box-shadow: 4px 4px 8px #888">
		</td>
		<td><b>Transductive Zero-shot Action Recognition via Visually-connected Graph Convolutional Networks</b> <br>
		<b>Yangyang Xu</b>, Chu Han, Jing Qin, Xuemiao Xu, Guoqiang Han, and Shengfeng He<br>
		<i><b>IEEE TNNLS 2020</b></i>  
		<br>
    <a href="https://ieeexplore.ieee.org/document/9173643">PDF</a> / 
    Supp / 
    <a href="https://github.com/cnnlstm/GAGCN">Code</a>  
		</td>

		<tr>
		<td width="206">
		<img src="images/da.png" width="200px" height = "75	" style="box-shadow: 4px 4px 8px #888">
		</td>
		<td><b>Unsupervised Domain Adaptation via Importance Sampling </b> <br>
		Xuemiao Xu, Hai He, Huaidong Zhang, <b>Yangyang Xu</b>, and Shengfeng He<br>
		<i><b>IEEE TCSVT 2019</b></i>  
		<br>
    <a href="https://ieeexplore.ieee.org/document/8946732">PDF</a> / 
    Supp / 
    Code</a>  
		</td>

		<tr>
		<td width="206">
		<img src="images/spl.png" width="200px" height = "75	" style="box-shadow: 4px 4px 8px #888">
		</td>
		<td><b>Ensemble One-Dimensional Convolution Neural Networks for Skeleton-Based Action Recognition </b> <br>
		<b>Yangyang Xu</b>, Jun Cheng, Lei Wang, Feng Liu and Dapeng Tao<br>
		<i><b>IEEE SPL 2018</b></i>  
		<br>
    <a href="https://ieeexplore.ieee.org/document/8368136">PDF</a> / 
    Supp /
    <a href="https://github.com/cnnlstm/Ensem-NN">Code</a> / 
    </td>


		<tr>
		<td width="206">
		<img src="images/access.png" width="200px" height = "75	" style="box-shadow: 4px 4px 8px #888">
		</td>
		<td><b>Human Action Recognition by Learning Spatio-Temporal Features With Deep Neural Networks </b> <br>
		Lei Wang, <b>Yangyang Xu</b>, Jun Cheng, Jianqin Yin and Jiaji Wu<br>
		<i><b>IEEE Access 2018</b></i>  
		<br>
    <a href="https://ieeexplore.ieee.org/document/8319974">PDF</a> / 
    Supp / 
    Code 
    </td>

		<tr>
		<td width="206">
		<img src="images/dta.png" width="200px" height = "75	" style="box-shadow: 4px 4px 8px #888">
		</td>
		<td><b>DTA: Double LSTM with temporal-wise attention network for action recognition </b> <br>
		<b>Yangyang Xu</b>, Lei Wang, Jun Cheng and Jiaji Wu<br>
		<i><b>ICCC 2017</b></i>  
		<br>
    <a href="https://ieeexplore.ieee.org/document/8322825">PDF</a> / 
    Supp / 
    Code 
    </td>




</tbody></table>

<h2><font> Professional Activities </font></h2>
<h3> Reviewer </h3>
<ul>
<li> 
<em><b>NeurIPS</em></b>, <em><b>ICML</em></b>, <em><b>SIGGRAPH</em></b>, <em><b>SIGGRAPH Asia</em></b>, <em><b>CVPR</em></b>, <em><b>ICCV</em></b>, <em><b>ECCV</em></b>, <em><b>AAAI</em></b>, <em><b>P&G</em></b>.
</li>
<li> <em><b>IEEE TIP</em></b>, <em><b>IEEE TNNLS</em></b>, <em>Pattern Recognition</em>, <em>Neural Computing</em>, <em><b>IEEE SPL</em></b>. 
</li>
</p> 
</li>
</ul>

<h3> Seminar Report </h3>
<ul>
<li>
<td><b>Graph Convolutional Neural Networks for Zero-shot Action Recognition</b> <br>
at City University of Hong Kong, Hong Kong. 2018.12. 
</li>
<li>
<td><b>Research on Several Problems Based on Generative Adversarial Models</b> <br>
at Tencent, Shenzhen. 2021.11 
</li>
<li>
<td><b>Image and Video Editing Based on Generative Adversarial Networks</b> <br>
at Shanghai AI Lab, Shanghai. 2022.06 
</li>
</ul>
<h3> Volunteer </h3>
<ul>
<li>
<b>Chinagraph</b> 2018    
</li>
</ul>


<!-- 
<a href="https://www.easycounter.com/">
<img src="https://www.easycounter.com/counter.php?yangyangxu"
border="0" alt="Hit Counters"></a>
<br><a href="https://www.easycounter.com/">Web Site Hit Counter</a> -->

<div>
  <p><center>
        <br>
            &copy; Yangyang Xu | Last updated: July, 2023. 
        </center></p>
</div>


<div style="text-align:center">
<img src="https://www.easycounter.com/counter.php?yangyangxu" border="10" alt="Hit Counters"></a>
</div>
</body></html>
</html>
