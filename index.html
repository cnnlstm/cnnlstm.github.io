<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />

<meta name="keywords" content="Yangyang, Yangyang Xu, CS, HKU, The University of Hong Kong">
<meta name="description" content="Yuanfeng Ji&#39;s home page">
<meta name="google-site-verification" content="X2QFrl-bPeg9AdlMt4VKT9v6MJUSTCf-SrY3CvKt4Zs" />
<link rel="stylesheet" href="jemdoc.css" type="text/css">
<link rel="shortcut icon" type="image/png" href="images/tab.ico" />

<title>Yangyang Xu&#39;s Homepage</title>
<!-- Google Analytics -->
<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-159069803-1', 'auto');
ga('send', 'pageview');
</script>
<!-- End Google Analytics -->
<!--
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-87320911-1', 'auto');
  ga('send', 'pageview');

</script>
-->
</head>
<body>
<div id="layout-content" style="margin-top:25px">
<table>
	<tbody>
		<tr>
			<td width="650">
				<div id="toptitle">
					<h1>Yangyang Xu</h1><h1>
					<h1><font face="Arial"> 徐洋洋 </font></h1>
				</h1></div>

				<h3>Associate Professor</h3> 
				<p>
					School of Intelligence Science and Engineering <br>
					Harbin Institute of Technology <br>
					Shenzhen, China<br>
<!-- 					<br>
					Email: cnnlstm[at]gmail.com -->

		<p style="text-align:left">
    <a href="mailto:cnnlstm@gmail.com"> Email</a> &nbsp/&nbsp
    <a href="https://scholar.google.com/citations?user=SmlxBFAAAAAJ&hl=en">Google Scholar
    <!-- </a> &nbsp/&nbsp -->
<!--     <a href="https://cnnlstm.github.io/YangyangXu_cv_open.pdf">CV</a> &nbsp/&nbsp
    <a href="https://cnnlstm.github.io/images/wechat.jpg">WeChat</a>
 -->
  	</p>
				</p>
				</p>
			</td>
			<td>
				<img src="./images/me.gif" border="0" width="225"><br>
			</td>
		</tr><tr>
	</tr></tbody>
</table>

<!--<h2>Biography [<a href="./CV-JinYueming.pdf">CV</a>]</h2>-->
<h2>Biography </h2>
<!-- <p>
	Yangyang Xu is an associated professor in Harbin Institute of Technology, Shenzhen. Before that, he is a postdoctoral researcher with Prof. <a href="https://eng.ox.ac.uk/people/tingting-zhu//">Tingting Zhu</a> in The University of Oxford from 2024 to 2025, and worked as a postdoctoral under the supervision of Prof. <a href="http://luoping.me/">Ping Luo</a> at The University of Hong Kong</a>. Before that, he receives his doctorate degree under the supervision of Prof. <a href="https://shengfenghe.com/">Shengfeng He</a> and Prof. <a href="https://scut-mm.github.io/people.html">Xuemiao Xu</a> at the South China University of Technology.
 -->
<p>
    Yangyang Xu is an Associate Professor at the Harbin Institute of Technology, Shenzhen. Prior to this, he was a Postdoctoral Researcher with Prof. <a href="https://eng.ox.ac.uk/people/tingting-zhu">Tingting Zhu</a> at the University of Oxford from 2024 to 2025. He also held a Postdoctoral position under the supervision of with Prof. <a href="http://luoping.me/">Ping Luo</a> at the University of Hong Kong from 2021 to 2024. He earned his Ph.D. at the South China University of Technology, under the supervision of Prof. <a href="https://shengfenghe.github.io/">Shengfeng He</a> and Prof. <a href="https://scut-mm.github.io/people.html">Xuemiao Xu</a>.
</p>
<p>
	His research primarily focuses on vision generative models, encompassing the understanding, control, design, and generalization of these models.
</p>
	<h2>News</h2>
<ul>
		<li>07/2025: One paper is accepted by ACM MM. </li>
		<li>06/2025: Three papers are accepted by ICCV. </li>
		<li>01/2025: One paper is accepted by IJCV and two papers are accepted by AAAI. </li>

<!--     <li>02/2023: One Paper is accepted by TOG. </li>
		<li>07/2022: One Paper is accepted by TIP. </li>
    <li>03/2022: One Paper is accepted by CVPR. </li>
    <li>12/2021: One Paper is accepted by TIP. </li>
    <li>12/2021: Join the department of computer science, HKU, and start the postdoctoral phase.</li>
    <li>08/2021: Passed the oral defense and became a <span style="color:rgb(224, 145, 92)"><b>Dr</b></span>.</li>
    <li>07/2021: One Paper is accepted by ICCV.</li>
    <li>06/2021: One Paper is accepted by TIP.</li> 
 --></ul>


<h2><font> Publications </font> </h2>
<table id="tbPublications" width="100%">
	<tbody>

		<div style="display: flex; align-items: center; margin: 20px 0;">
        <hr style="flex: 1; border: none; height: 1px; background-color: #ccc; margin: 0 10px;">
        <span style="white-space: nowrap; font-weight: bold;">2025</span>
        <hr style="flex: 1; border: none; height: 1px; background-color: #ccc; margin: 0 10px;">
    </div>

    <tr>
		<td width="206">
		<img src="images/mm-2025.png" width="200px" height = "75	" style="box-shadow: 4px 4px 8px #888">
		</td>
		<td><b>DiffusionMat: Alpha Matting as Deterministic Sequential Refinement Learning</b> <br>
		<b>Yangyang Xu</b>, Shengfeng He, Wenqi Shao, Yong Du, Kwan-Yee K. Wong, Yu Qiao, Jun Yu, Ping Luo<br>
		<i><b>ACM MM 2025</b></i>  
		<br>
		PDF / 
		Supp / 
		<a href="https://github.com/cnnlstm/DiffusionMat">Code</a> 
		</td>

    <tr>
		<td width="206">
		<img src="images/iccv-25a.png" width="200px" height = "75	" style="box-shadow: 4px 4px 8px #888">
		</td>
		<td><b>Cross-Subject Mind Decoding from Inaccurate Representations</b> <br>
		<b>Yangyang Xu</b>, Bangzhen Liu, Wenqi Shao, Yong Du, Shengfeng He, Tingting Zhu<br>
		<i><b>ICCV 2025</b></i>  
		<br>
		PDF / 
		Supp / 
		Project
		</td>


    <tr>
		<td width="206">
		<img src="images/iccv-25b.png" width="200px" height = "75	" style="box-shadow: 4px 4px 8px #888">
		</td>
		<td><b>Stable Score Distillation</b> <br>
		Haiming Zhu, <b>Yangyang Xu</b>*, Chenshu Xu, Tingrui Shen, Wenxi Liu, Yong Du, Jun Yu, Shengfeng He<br>
		<i><b>ICCV 2025</b></i>  
		<br>
		PDF / 
		Supp / 
		Project
		</td>

    <tr>
		<td width="206">
		<img src="images/iccv-25c.png" width="200px" height = "75	" style="box-shadow: 4px 4px 8px #888">
		</td>
		<td><b>OmniVTON: Training-Free Universal Virtual Try-On</b> <br>
		Zhaotong Yang, Yuhui Li, Shengfeng He, Xinzhe Li, <b>Yangyang Xu</b>, Junyu Dong, Yong Du<br>
		<i><b>ICCV 2025</b></i>  
		<br>
		PDF / 
		Supp / 
		Project
		</td>

    <tr>
		<td width="206">
		<img src="images/pr.png" width="200px" height = "75	" style="box-shadow: 4px 4px 8px #888">
		</td>
		<td><b>ContX: Scene context prediction via context bank and layout perception</b> <br>
		Jingxin Liang+, <b>Yangyang Xu</b>+, Haorui Song, Yuqin Lu, Yuhui Deng, Yiyi Long, Yan Huang, Shengxin Liu, Jianbo Jiao, Shengfeng He<br>
		<i><b>PR 2025</b></i>  
		<br>
		<a href="https://www.sciencedirect.com/science/article/pii/S0031320325005126">PDF</a> / 
		Supp / 
		<a href="https://github.com/liangjingxin4747/ContX/">Code</a>  
		</td>


    <tr>
		<td width="206">
		<img src="images/ijcv.png" width="200px" height = "75	" style="box-shadow: 4px 4px 8px #888">
		</td>
		<td><b>RIGID: Recurrent GAN Inversion and Editing of Real Face Videos and Beyond</b> <br>
		<b>Yangyang Xu</b>,  Shengfeng He, Kwan-Yee K. Wong, Ping Luo<br>
		<i><b>IJCV 2025</b></i>  
		<br>
		<a href="https://link.springer.com/article/10.1007/s11263-024-02329-8">PDF</a> / 
		Supp / 
		<a href="https://cnnlstm.github.io/RIGID/">Project</a>  
		</td>


    <tr>
		<td width="206">
		<img src="images/aaai-25a.jpg" width="200px" height = "75	" style="box-shadow: 4px 4px 8px #888">
		</td>
		<td><b>Occlusion-Insensitive Talking Head Video Generation via Facelet Compensation</b> <br>
		Yuhui Deng, Yuqin Lu, <b>Yangyang Xu</b>, Yongwei Nie, Shengfeng He<br>
		<i><b>AAAI 2025</b></i>  
		<br>
		<a href="https://ojs.aaai.org/index.php/AAAI/article/view/32277">PDF</a> /
		Supp / 
		Code
		</td>


    <tr>
		<td width="206">
		<img src="images/aaai-25c.jpg" width="200px" height = "75	" style="box-shadow: 4px 4px 8px #888">
		</td>
		<td><b>PersonaMagic: Stage-Regulated High-Fidelity Face Customization with Tandem Equilibrium</b> <br>
		Xinzhe Li, Jiahui Zhan, Shengfeng He, <b>Yangyang Xu</b>, Junyu Dong, Huaidong Zhang, Yong Du<br>
		<i><b>AAAI 2025</b></i>  
		<br>
		<a href="https://ojs.aaai.org/index.php/AAAI/article/view/32529">PDF</a> /
		Supp / 
		Code
		</td>

    </tr>
    </tbody>
		</table>

	<table id="tbPublications" width="100%">
	<tbody>


    <!-- 2024 -->
    <div style="display: flex; align-items: center; margin: 20px 0;">
        <hr style="flex: 1; border: none; height: 1px; background-color: #ccc; margin: 0 10px;">
        <span style="white-space: nowrap; font-weight: bold;">2024</span>
        <hr style="flex: 1; border: none; height: 1px; background-color: #ccc; margin: 0 10px;">
    </div>

    <tr>
		<td width="206">
		<img src="images/cvmj.jpg" width="200px" height = "75	" style="box-shadow: 4px 4px 8px #888">
		</td>
		<td><b>Learning Coherent Portrait-to-Anime Translation via Latent Cyclic Transformation
</b> <br>
		<b>Yangyang Xu</b>,  Shengfeng He, Kwan-Yee K. Wong, Ping Luo<br>
		<i><b>CVMJ 2024</b></i>  
		<br>
		PDF / 
		Supp / 
		Code
		</td>


    <tr>
		<td width="206">
		<img src="images/tvcg24.png" width="200px" height = "75	" style="box-shadow: 4px 4px 8px #888">
		</td>
		<td><b>DreamAnime: Learning Style-Identity Textual Disentanglement for Anime and Beyond
	</b> <br>
		Chenshu Xu, <b>Yangyang Xu</b>, Huaidong Zhang, Xuemiao Xu, Shengfeng He<br>
		<i><b>TVCG 2024</b></i>  
		<br>
		<a href="https://ieeexplore.ieee.org/document/10521816">PDF</a> / 
		Supp / 
		<a href="https://github.com/chnshx/DreamAnime/">Code</a>  
		</td>

    </tr>
    </tbody>
		</table>



	<table id="tbPublications" width="100%">
	<tbody>
		<div style="display: flex; align-items: center; margin: 20px 0;">
        <hr style="flex: 1; border: none; height: 1px; background-color: #ccc; margin: 0 10px;">
        <span style="white-space: nowrap; font-weight: bold;">2023</span>
        <hr style="flex: 1; border: none; height: 1px; background-color: #ccc; margin: 0 10px;">
    </div>

    <tr>
		<td width="206">
		<img src="images/iccv2.png" width="200px" height = "75	" style="box-shadow: 4px 4px 8px #888">
		</td>
		<td><b>RIGID: Recurrent GAN Inversion and Editing of Real Face Videos</b> <br>
		<b>Yangyang Xu</b>,  Shengfeng He, Kwan-Yee K. Wong, Ping Luo<br>
		<i><b>ICCV 2023</b></i>  
		<br>
		<a href="https://arxiv.org/pdf/2308.06097.pdf">PDF</a> / 
		Supp / 
		<a href="https://cnnlstm.github.io/RIGID/">Project</a>  
		</td>

    <tr>
		<td width="206">
		<img src="images/tog1.png" width="200px" height = "75	" style="box-shadow: 4px 4px 8px #888">
		</td>
		<td><b>Parsing-Conditioned Anime Translation: A New Dataset and Method</b> <br>
		Zhansheng Li+, <b>Yangyang Xu</b>+,  Nanxuan Zhao, Yang Zhou, Yongtuo Liu, Dahua Lin, Shengfeng He<br>
		<i><b>ACM TOG 2023</b></i>  
		<br>
		<a href="https://dl.acm.org/doi/10.1145/3585002">PDF</a> / 
		Supp / 
		<a href="https://github.com/zsl2018/StyleAnime/">Code</a>  
		</td>
    </tr>
    </tbody>
		</table>


	<table id="tbPublications" width="100%">
	<tbody>
		<div style="display: flex; align-items: center; margin: 20px 0;">
        <hr style="flex: 1; border: none; height: 1px; background-color: #ccc; margin: 0 10px;">
        <span style="white-space: nowrap; font-weight: bold;">2022</span>
        <hr style="flex: 1; border: none; height: 1px; background-color: #ccc; margin: 0 10px;">
    </div>

    <tr>
		<td width="206">
		<img src="images/mat.png" width="200px" height = "75	" style="box-shadow: 4px 4px 8px #888">
		</td>
		<td><b>Self-supervised Matting-specific Portrait Enhancement and Generation</b> <br>
		<b>Yangyang Xu</b>,  Zeyang Zhou, Shengfeng He<br>
		<i><b>IEEE TIP 2022</b></i>  
		<br>
		<a href="https://ieeexplore.ieee.org/document/9849440">PDF</a> / 
		Supp / 
		<a href="https://github.com/cnnlstm/StyleGAN_Matting">Code</a>  
		</td>


    <tr>
		<td width="206">
		<img src="images/faceswap.png" width="200px" height = "75	" style="box-shadow: 4px 4px 8px #888">
		</td>
		<td><b>High-resolution Face Swapping via Latent Semantics Disentanglement</b> <br>
		<b>Yangyang Xu</b>,  Bailin Deng, Junle Wang, Yanqing Jing, Jia Pan, Shengfeng He<br>
		<i><b>CVPR 2022</b></i>  
		<br>
		<a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Xu_High-Resolution_Face_Swapping_via_Latent_Semantics_Disentanglement_CVPR_2022_paper.pdf">PDF</a> / 
		<a href="https://openaccess.thecvf.com/content/CVPR2022/supplemental/Xu_High-Resolution_Face_Swapping_CVPR_2022_supplemental.pdf">Supp</a> / 
		<a href="https://github.com/cnnlstm/FSLSD_HiRes">Code</a>  
		</td>


		<tr>
		<td width="206">
		<img src="images/propulse.png" width="200px" height = "75	" style="box-shadow: 4px 4px 8px #888">
		</td>
		<td><b>Pro-PULSE: Learning Progressive Encoders of Latent Semantics in GANs for Photo Upsampling</b> <br>
		Yang Zhou+, <b>Yangyang Xu</b>+, Yong Du, Qiang Wen, Shengfeng He<br>
		<i><b>IEEE TIP 2022</b></i>  
		<br>
    <a href="https://ieeexplore.ieee.org/document/9678071">PDF</a>  /
    Supp / 
    Code 
		</td>


		<tr>
		<td width="206">
		<img src="images/icme.png" width="200px" height = "75	" style="box-shadow: 4px 4px 8px #888">
		</td>
		<td><b>Background Matting via Recursive Excitation</b> <br>
		Junjie Deng+, <b>Yangyang Xu</b>+, Zeyang Zhou, Shengfeng He<br>
		<i><b>ICME 2022</b></i>  
		<br>
    <a href="https://ieeexplore.ieee.org/document/9859876/">PDF</a>  / 
    Supp / 
    <a href="https://github.com/csdjj/EnhancedBGM/">Code</a> 
		</td>


		<tr>
		<td width="206">
		<img src="images/aod.png" width="200px" height = "75	" style="box-shadow: 4px 4px 8px #888">
		</td>
		<td><b>Representative Feature Alignment for Adaptive Object Detection</b> <br>
		Shan Xu, Huaidong Zhang, Xuemiao Xu, Xiaowei Hu, <b>Yangyang Xu</b>, Liangui Dai, Kup-Sze Choi, Pheng-Ann Heng<br>
		<i><b>IEEE TCSVT 2022</b></i>  
		<br>
    <a href="https://ieeexplore.ieee.org/abstract/document/9868052/">PDF</a>  / 
    Supp / 
    Code
		</td>
    </tr>
    </tbody>
		</table>


	<table id="tbPublications" width="100%">
	<tbody>
		<div style="display: flex; align-items: center; margin: 20px 0;">
        <hr style="flex: 1; border: none; height: 1px; background-color: #ccc; margin: 0 10px;">
        <span style="white-space: nowrap; font-weight: bold;">2021</span>
        <hr style="flex: 1; border: none; height: 1px; background-color: #ccc; margin: 0 10px;">
    </div>


		<tr>
		<td width="206">
		<img src="images/inversion.png" width="200px" height = "75	" style="box-shadow: 4px 4px 8px #888">
		</td>
		<td><b>From Continuity to Editability: Inverting GANs with Consecutive Images</b> <br>
		<b>Yangyang Xu</b>, Yong Du, Wenpeng Xiao, Xuemiao Xu, Shengfeng He<br>
		<i><b>ICCV 2021</b></i>  
		<br>
    <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Xu_From_Continuity_to_Editability_Inverting_GANs_With_Consecutive_Images_ICCV_2021_paper.pdf">PDF</a> / 
    <a href="https://openaccess.thecvf.com/content/ICCV2021/supplemental/Xu_From_Continuity_to_ICCV_2021_supplemental.pdf">Supp</a> / 
    <a href="https://github.com/cnnlstm/InvertingGANs_with_ConsecutiveImgs">Code</a>  
		</td>


		<tr>
		<td width="206">
		<img src="images/faceflow.png" width="200px" height = "75	" style="box-shadow: 4px 4px 8px #888">
		</td>
		<td><b>Multi-view Face Synthesis via Progressive Face Flow</b> <br>
		<b>Yangyang Xu</b>, Xuemiao Xu, Jianbo Jiao, Keke Li, Cheng Xu, Shengfeng He<br>
		<i><b>IEEE TIP 2021</b></i>  
		<br>
    <a href="https://ieeexplore.ieee.org/document/9466401">PDF</a> / 
    <a href="https://ieeexplore.ieee.org/document/9495995">Erratum</a> / 
    Code      
		</td>


		<tr>
		<td width="206">
		<img src="images/ham.png" width="200px" height = "75	" style="box-shadow: 4px 4px 8px #888">
		</td>
		<td><b>Holistically-Associated Transductive Zero-Shot Learning</b> <br>
		<b>Yangyang Xu</b>, Xuemiao Xu, Guoqiang Han, Shengfeng He<br>
		<i><b>IEEE TCDS 2021</b></i>  
		<br>
    <a href="https://ieeexplore.ieee.org/document/9314882">PDF</a> / 
    Supp / 
    Code 
		</td>


		<tr>
		<td width="206">
		<img src="images/tomm.png" width="200px" height = "75	" style="box-shadow: 4px 4px 8px #888">
		</td>
		<td><b>Invertible Grayscale with Sparsity Enforcing Priors</b> <br>
		Yong Du, <b>Yangyang Xu</b>, Taizhong Ye, Qiang Wen, Chufeng Xiao, Junyu Dong, Guoqiang Han, Shengfeng He<br>
		<i><b>ACM TOMM 2021</b></i>  
		<br>
    <a href="https://dl.acm.org/doi/10.1145/3451993">PDF</a> / 
    Supp / 
    Code 
		</td>


		<tr>
		<td width="206">
		<img src="images/cod.png" width="200px" height = "75	" style="box-shadow: 4px 4px 8px #888">
		</td>
		<td><b>Deep Texture-Aware Features for Camouflaged Object Detection</b> <br>
		Jingjing Ren, Xiaowei Hu, Lei Zhu, Xuemiao Xu, <b>Yangyang Xu</b>, Weiming Wang, Zijun Deng, Pheng-Ann Heng<br>
		<i><b>IEEE TCSVT 2021</b></i>  
		<br>
    <a href="https://ieeexplore.ieee.org/document/9606888">PDF</a> / 
    Supp / 
    Code 
		</td>
		</tr>
    </tbody>
		</table>


		<table id="tbPublications" width="100%">
		<tbody>
		<div style="display: flex; align-items: center; margin: 20px 0;">
        <hr style="flex: 1; border: none; height: 1px; background-color: #ccc; margin: 0 10px;">
        <span style="white-space: nowrap; font-weight: bold;">2020</span>
        <hr style="flex: 1; border: none; height: 1px; background-color: #ccc; margin: 0 10px;">
    </div>

		<tr>
		<td width="206">
		<img src="images/gagcn.png" width="200px" height = "75	" style="box-shadow: 4px 4px 8px #888">
		</td>
		<td><b>Transductive Zero-shot Action Recognition via Visually-connected Graph Convolutional Networks</b> <br>
		<b>Yangyang Xu</b>, Chu Han, Jing Qin, Xuemiao Xu, Guoqiang Han, Shengfeng He<br>
		<i><b>IEEE TNNLS 2020</b></i>  
		<br>
    <a href="https://ieeexplore.ieee.org/document/9173643">PDF</a> / 
    Supp / 
    <a href="https://github.com/cnnlstm/GAGCN">Code</a>  
		</td>
		</tr>
    </tbody>
		</table>


		<table id="tbPublications" width="100%">
		<tbody>
		<div style="display: flex; align-items: center; margin: 20px 0;">
        <hr style="flex: 1; border: none; height: 1px; background-color: #ccc; margin: 0 10px;">
        <span style="white-space: nowrap; font-weight: bold;">2019</span>
        <hr style="flex: 1; border: none; height: 1px; background-color: #ccc; margin: 0 10px;">
    </div>

		<tr>
		<td width="206">
		<img src="images/da.png" width="200px" height = "75	" style="box-shadow: 4px 4px 8px #888">
		</td>
		<td><b>Unsupervised Domain Adaptation via Importance Sampling </b> <br>
		Xuemiao Xu, Hai He, Huaidong Zhang, <b>Yangyang Xu</b>, Shengfeng He<br>
		<i><b>IEEE TCSVT 2019</b></i>  
		<br>
    <a href="https://ieeexplore.ieee.org/document/8946732">PDF</a> / 
    Supp / 
    Code</a>  
		</td>
		</tr>
    </tbody>
		</table>


		<table id="tbPublications" width="100%">
		<tbody>
		<div style="display: flex; align-items: center; margin: 20px 0;">
        <hr style="flex: 1; border: none; height: 1px; background-color: #ccc; margin: 0 10px;">
        <span style="white-space: nowrap; font-weight: bold;">2018</span>
        <hr style="flex: 1; border: none; height: 1px; background-color: #ccc; margin: 0 10px;">
    </div>


		<tr>
		<td width="206">
		<img src="images/spl.png" width="200px" height = "75	" style="box-shadow: 4px 4px 8px #888">
		</td>
		<td><b>Ensemble One-Dimensional Convolution Neural Networks for Skeleton-Based Action Recognition </b> <br>
		<b>Yangyang Xu</b>, Jun Cheng, Lei Wang, Feng Liu, Dapeng Tao<br>
		<i><b>IEEE SPL 2018</b></i>  
		<br>
    <a href="https://ieeexplore.ieee.org/document/8368136">PDF</a> / 
    Supp /
    <a href="https://github.com/cnnlstm/Ensem-NN">Code</a> / 
    </td>


		<tr>
		<td width="206">
		<img src="images/access.png" width="200px" height = "75	" style="box-shadow: 4px 4px 8px #888">
		</td>
		<td><b>Human Action Recognition by Learning Spatio-Temporal Features With Deep Neural Networks </b> <br>
		Lei Wang, <b>Yangyang Xu</b>, Jun Cheng, Jianqin Yin, Jiaji Wu<br>
		<i><b>IEEE Access 2018</b></i>  
		<br>
    <a href="https://ieeexplore.ieee.org/document/8319974">PDF</a> / 
    Supp / 
    Code 
    </td>
		</tr>
    </tbody>
		</table>



		<table id="tbPublications" width="100%">
		<tbody>
		<div style="display: flex; align-items: center; margin: 20px 0;">
        <hr style="flex: 1; border: none; height: 1px; background-color: #ccc; margin: 0 10px;">
        <span style="white-space: nowrap; font-weight: bold;">2017</span>
        <hr style="flex: 1; border: none; height: 1px; background-color: #ccc; margin: 0 10px;">
    </div>
		<tr>
		<td width="206">
		<img src="images/dta.png" width="200px" height = "75	" style="box-shadow: 4px 4px 8px #888">
		</td>
		<td><b>DTA: Double LSTM with temporal-wise attention network for action recognition </b> <br>
		<b>Yangyang Xu</b>, Lei Wang, Jun Cheng, Jiaji Wu<br>
		<i><b>ICCC 2017</b></i>  
		<br>
    <a href="https://ieeexplore.ieee.org/document/8322825">PDF</a> / 
    Supp / 
    Code 
    </td>
		</tr>
    </tbody>
		</table>



</tbody></table>

<h2><font> Professional Activities </font></h2>
<h3> Reviewer </h3>
<ul>
<li> 
<em><b>NeurIPS</em></b>, <em><b>ICML</em></b>, <em><b>ICLR</em></b>, <em><b>SIGGRAPH</em></b>, <em><b>SIGGRAPH Asia</em></b>, <em><b>CVPR</em></b>, <em><b>ICCV</em></b>, <em><b>ECCV</em></b>, <em><b>AAAI</em></b>, <em><b>IJCAI</em></b>, <em><b>P&G</em></b>.
</li>
<li> <em><b>IEEE TPAMI</em></b>, <em><b>IEEE IJCV</em></b>, <em><b>IEEE TIP</em></b>, <em><b>IEEE TNNLS</em></b>, <em>Pattern Recognition</em>, <em>Neural Computing</em>, <em><b>IEEE SPL</em></b>. 
</li>
</p> 
</li>
</ul>

<h3> Seminar Report </h3>
<ul>

<li>
<td><b>Portrait Editing and Matting In Generative Way</b> <br>
at CMU, Online. 2024.03 
</li>

<li>
<td><b>Image and Video Processing using Generative Models</b> <br>
at VITA Lab in EPFL, Online. 2023.02 
</li>

<li>
<td><b>Image and Video Editing Based on Generative Adversarial Networks</b> <br>
at Shanghai AI Lab, Shanghai. 2022.06 
</li>

<li>
<td><b>Research on Several Problems Based on Generative Adversarial Models</b> <br>
at Tencent, Shenzhen. 2021.11 
</li>



<li>
<td><b>Graph Convolutional Neural Networks for Zero-shot Action Recognition</b> <br>
at City University of Hong Kong, Hong Kong. 2018.12. 
</li>






</ul>
<h3> Volunteer </h3>
<ul>
<li>
<b>Chinagraph</b> 2018    
</li>
</ul>


<!-- 
<a href="https://www.easycounter.com/">
<img src="https://www.easycounter.com/counter.php?yangyangxu"
border="0" alt="Hit Counters"></a>
<br><a href="https://www.easycounter.com/">Web Site Hit Counter</a> -->

<div>
  <p><center>
        <br>
            &copy; Yangyang Xu | Last updated: July, 2025. 
        </center></p>
</div>


<div style="text-align:center">
<img src="https://www.easycounter.com/counter.php?yangyangxu" border="10" alt="Hit Counters"></a>
</div>
</body></html>
</html>
