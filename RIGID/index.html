<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <title>RIGID</title>
    <meta name="description" content="Project page of RIGID paper, 2022">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <link rel="icon" type="image/png" href="eccv_logo.png">
    <!-- Format -->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="../format/app.css">
    <link rel="stylesheet" href="../format/bootstrap.min.css">
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>
    <script src="../format/app.js"></script>

  </head>

  <body>
    <div class="container" id="main">
        <div class="row">
            <h1 class="col-md-12 text-center">
               RIGID: Recurrent GAN Inversion and Editing of Real Face Videos  <br /> 
         
<!--                 <small>
                    
                </small> -->
            </h1>
        </div>
        
<!--         <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                    <li>
                        <a href="https://tengfei-wang.github.io/" >
                         Tengfei Wang
                        </a>
                        <br /> HKUST
                        <br /> &nbsp
                    </li>

                    <li>
                        <a href="https://yzhang2016.github.io/yongnorriszhang.github.io/">
                            Yong Zhang
                      </a>
                        <br />Tencent AI Lab
                      <br /> &nbsp
                    </li>

                    <li>
                        <a href="https://sites.google.com/site/yanbofan0124/">
                            Yanbo Fan
                      </a>
                        <br />Tencent AI Lab
                      <br /> &nbsp
                    </li>
              
                    <li>
                        <a href="https://juew.org/">
                           Jue Wang
                        </a>
                        <br />Tencent AI Lab
                      <br /> &nbsp
                    </li>
                  
                    <li>
                        <a href="https://cqf.io/">
                           Qifeng Chen
                        </a>
                        <br /> HKUST 
                        <br /> &nbsp
                    </li>
                </ul>
            </div>
        </div> -->



        <!-- ##### Elements #####-->
        <!-- <div class="row">
                <div class="col-md-8 col-md-offset-2 text-center">
                    <ul class="nav nav-pills nav-justified">
                        <li>
                            <a href="https://arxiv.org/abs/2109.06590">
                            <img src="./paper.jpg" height="100px"><br>
                                <h4><strong>Paper</strong></h4>
                            </a>
                        </li>
                        <li>

                            <a href="https://www.youtube.com/watch?v=yfF9QdIsbvU">
                            <img src="../images/youtube_icon.jpg" height="100px"><br>
                                <h4><strong>Video</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="https://github.com/Tengfei-Wang/HFGI">
                            <img src="../images/github_icon.jpg" height="100px"><br>
                                <h4><strong>Code</strong></h4>
                            </a>
                        </li>

                        <li>
                            <a href="http://colab.research.google.com/github/Tengfei-Wang/HFGI/blob/main/HFGI_playground.ipynb">
                            <img src="../images/colab_icon.jpg" height="100px"><br>
                                <h4><strong>Colab</strong></h4>
                            </a>
                        </li>
 
 
                        <li>
                            <a href="https://github.com/Tengfei-Wang/Tengfei-Wang.github.io/blob/master/HFGI/supp.pdf">
                            <img src="../images/slide_icon.jpg" height="100px"><br>
                                <h4><strong>Supp</strong></h4>
                            </a>
                        </li>                     
                      
                      
                    </ul>
                </div>
        </div>
 -->

        <!-- ##### Abstract #####-->
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
<!--                 It seems that arxiv fails to properly compile the pdf file, so I put the pdf file in the repo as a remedy:( -->
<!--               
                Want to play with your photos without cloning the code? Try our <a href="https://replicate.com/tengfei-wang/hfgi"> ONLINE DEMO </a> for fun! -->
                <h3>
                    Abstract
                </h3>
                <p class="text-justify">
                GAN inversion is indispensable for applying the powerful editability of GAN to real images. However, existing methods invert video frames individually often lead to undesired inconsistent results over time. In this paper, we propose a unify recurrent framework, named \textbf{R}ecurrent v\textbf{I}deo \textbf{G}AN \textbf{I}nversion and e\textbf{D}iting (RIGID), to explicitly and simultaneously enforce temporally coherent GAN inversion and facial editing of real videos. Our approach models the temporal relations between current and previous frames from three aspects. To enable a faithful real video reconstruction, we first maximize the inversion fidelity and consistency by learning a temporal compensated latent code. Second, we observe incoherence noises lie in high-frequency domain that can be disentangled from the latent space. Third, to remove the inconsistency after attribute manipulation, we propose an \textit{in-between frame composition constraint} such that the an arbitrary frame must be a direct composite of its neighboring frames. Our unify framework learns the inherent coherence between input frames in an end-to-end manner, and therefore it is agnostic to a specific attribute and can be applied to arbitrary editing of the same video without re-training. Extensive experiments demonstrate that RIGID outperforms state-of-the-art methods qualitatively and quantitatively in both inversion and editing tasks.  
                </p>
            </div>
        </div>



    <div class="row">      
     <div class="col-md-8 col-md-offset-2">
          <h3>
              Results on High-Fidelity Video Editing (+ Smile)
          </h3>   
    </div>   
      
    <table width="90%" style="margin: 15pt auto; text-align: center;">      
      <tr>
          <td>
            <video width="800"  controls >
                <source src="./video_editing/1.mp4" type="video/mp4">
              Your browser does not support HTML video.
            </video>
          </td>
      </tr>     

            <tr>
          <td>
            <video width="800"  controls >
                <source src="./video_editing/2.mp4" type="video/mp4">
              Your browser does not support HTML video.
            </video>
          </td>
      </tr>  
    </table>          
      </div>
          


        <!-- ##### Approach #####-->
        <div class="row">
          <div class="col-md-8 col-md-offset-2">
              <h3>
                Approach 
              </h3>
              <p class="text-justify">
              Overview of our high-fidelity image inversion and editing framework. The basic 
              encoder E<sub>0</sub> infers a low-rate latent code W corresponding to a low-fidelity reconstruction
                image  hat{X}<sub>o</sub>. The distortion map contains the lost high-frequency image-specific 
                details to improve the reconstruction fidelity. The red dotted boxes indicate the editing behaviour
                with certain semantic direction.  To achieve high-fidelity image editing, we propose the distortion 
                consultation branch to facilitate the generation. In the distortion consultation, &Delta;
                is first aligned with the low-fidelity edited image  by ADA and then embedded to a high-rate 
                latent map C via the consultation encoder  E<sub>c</sub>. Latent code W and latent map C are combined 
                via the consultation fusion (see details in the right part) across layers of G<sub>0</sub> to generate
                the final edited image.
              </p>
            <img src="./method.jpg" class="img-responsive" alt="method" class="center"><br>

          </div>
        </div>
    
    
         <div class="row">
          <div class="col-md-8 col-md-offset-2">
                <h3>
                    More Results 
                </h3>   
            
          <img src="./real_edit.jpg" class="img-responsive" alt="method" class="center"><br>
           
          </div>
        </div>     
    
    
        <!-- ##### BibTex #####-->
        <hr>
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    BibTeX
                </h3>
 
                <div class="row align-items-center">
                    <div class="col py-3">
                        <pre class="border">             
@inproceedings{wang2021HFGI,
  title={High-Fidelity GAN Inversion for Image Attribute Editing},
  author={Wang, Tengfei and Zhang, Yong and Fan, Yanbo and Wang, Jue and Chen, Qifeng},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2022}
}
</pre>
                    </div>
                </div>
              
    
          </div>
          
        </div>

    </div>
</body>
</html>
